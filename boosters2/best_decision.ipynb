{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(196056, 43)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('onetwotrip_challenge_train.csv', index_col=False)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(455011, 37)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('onetwotrip_challenge_test.csv', index_col=False)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train.goal1\n",
    "train = train[test.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(651067, 37)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full = pd.concat([train, test])\n",
    "full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add my features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def months_delation(raw):\n",
    "    delation = raw.field3 - raw.field2\n",
    "    if delation < 0:\n",
    "        return 12 + delation\n",
    "    return delation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_year(raw):\n",
    "    delation = raw.field3 - raw.field2\n",
    "    if delation < 0:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['months_delation'] = train.apply(months_delation, axis=1)\n",
    "train['new_year'] = train.apply(new_year, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['months_delation'] = test.apply(months_delation, axis=1)\n",
    "test['new_year'] = test.apply(new_year, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add EDA features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique14 = np.unique(full['field14'])\n",
    "new_unique14 = np.round(unique14 / 0.14006639 - 0.27860731).astype(int)\n",
    "dict14 = {unique14[i]: new_unique14[i] for i in range(len(unique14))}\n",
    "train['field14_new'] = train['field14'].apply(lambda x: dict14[x])\n",
    "test['field14_new'] = test['field14'].apply(lambda x: dict14[x])\n",
    "\n",
    "unique1 = np.unique(full['field1'])\n",
    "new_unique1 = np.round(unique1  / 0.077571 + 0.0765905).astype(int)\n",
    "dict1 = {unique1[i]: new_unique1[i] for i in range(len(unique1))}\n",
    "train['field1_new'] = train['field1'].apply(lambda x: dict1[x])\n",
    "test['field1_new'] = test['field1'].apply(lambda x: dict1[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_columns = train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add user statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_users_info = train.drop('orderid', axis=1).groupby('userid').agg(['sum', 'mean', 'max', 'min'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113902/113902 [03:13<00:00, 588.61it/s]\n"
     ]
    }
   ],
   "source": [
    "train_users_info_dict = {}\n",
    "\n",
    "for userid in tqdm(list(train.userid.unique())):\n",
    "    train_users_info_dict[userid] = {}\n",
    "    for (n1, n2), value in zip(train_users_info.loc[userid].index, train_users_info.loc[userid]):\n",
    "        train_users_info_dict[userid][f'user_{n1}_{n2}'] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_users_info = test.drop('orderid', axis=1).groupby('userid').agg(['sum', 'mean', 'max', 'min'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 264547/264547 [07:02<00:00, 626.63it/s]\n"
     ]
    }
   ],
   "source": [
    "test_users_info_dict = {}\n",
    "\n",
    "for userid in tqdm(list(test.userid.unique())):\n",
    "    test_users_info_dict[userid] = {}\n",
    "    for (n1, n2), value in zip(test_users_info.loc[userid].index, test_users_info.loc[userid]):\n",
    "        test_users_info_dict[userid][f'user_{n1}_{n2}'] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = list(train_users_info_dict['10d654494cbe97bbb25d51ead2600679aff9e097924add09d8066010a0c9adaf'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [00:52<00:00,  3.08it/s]\n"
     ]
    }
   ],
   "source": [
    "for c in tqdm(new_columns):\n",
    "    train[c] = train.userid.apply(lambda x: train_users_info_dict[x][c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [02:02<00:00,  1.34it/s]\n"
     ]
    }
   ],
   "source": [
    "for c in tqdm(new_columns):\n",
    "    test[c] = test.userid.apply(lambda x: test_users_info_dict[x][c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((196056, 197), (455011, 197))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train_with_users.csv', index=False)\n",
    "test.to_csv('test_with_users.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New feauture: количество заказов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_userid = train.userid.value_counts()\n",
    "test_userid = test.userid.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['userid_orders'] = train.userid.apply(lambda x: train_userid[x])\n",
    "test['userid_orders'] = test.userid.apply(lambda x: test_userid[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_columns = list(original_columns)\n",
    "original_columns += ['userid_orders']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add statistics features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orderid</th>\n",
       "      <th>userid</th>\n",
       "      <th>field0</th>\n",
       "      <th>field1</th>\n",
       "      <th>field2</th>\n",
       "      <th>field3</th>\n",
       "      <th>field4</th>\n",
       "      <th>field5</th>\n",
       "      <th>field6</th>\n",
       "      <th>field7</th>\n",
       "      <th>...</th>\n",
       "      <th>user_new_year_min</th>\n",
       "      <th>user_field14_new_sum</th>\n",
       "      <th>user_field14_new_mean</th>\n",
       "      <th>user_field14_new_max</th>\n",
       "      <th>user_field14_new_min</th>\n",
       "      <th>user_field1_new_sum</th>\n",
       "      <th>user_field1_new_mean</th>\n",
       "      <th>user_field1_new_max</th>\n",
       "      <th>user_field1_new_min</th>\n",
       "      <th>userid_orders</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10d654494cbe97bbb25d51ead2600679aff9e097924add...</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.626508</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>-8.000000</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4aafc0391f72bbcf60537aece62923baf9ce644b64ac36...</td>\n",
       "      <td>144</td>\n",
       "      <td>-0.393794</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>bac8ffef46348f587c8d17137ab01fb24aef21547c647d...</td>\n",
       "      <td>134</td>\n",
       "      <td>-0.548937</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>-7.500000</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0392247b4b87674aba2c32bf2292b105771a6a376871be...</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.238651</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-3.333333</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>-4.666667</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>d1aeefef311bbeb4bd84876c8d49421f276674527d5578...</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.704079</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>-9.000000</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 198 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   orderid                                             userid  field0  \\\n",
       "0        0  10d654494cbe97bbb25d51ead2600679aff9e097924add...       0   \n",
       "1        1  4aafc0391f72bbcf60537aece62923baf9ce644b64ac36...     144   \n",
       "2        2  bac8ffef46348f587c8d17137ab01fb24aef21547c647d...     134   \n",
       "3        3  0392247b4b87674aba2c32bf2292b105771a6a376871be...       0   \n",
       "4        4  d1aeefef311bbeb4bd84876c8d49421f276674527d5578...       0   \n",
       "\n",
       "     field1  field2  field3  field4  field5  field6  field7  ...  \\\n",
       "0 -0.626508      11      12       1       1       0       1  ...   \n",
       "1 -0.393794       5       7       2       0       0       2  ...   \n",
       "2 -0.548937       2       3       2       0       0       1  ...   \n",
       "3 -0.238651      10      11       1       1       3       2  ...   \n",
       "4 -0.704079       8      11       1       1       0       1  ...   \n",
       "\n",
       "   user_new_year_min  user_field14_new_sum  user_field14_new_mean  \\\n",
       "0                0.0                  -5.0              -5.000000   \n",
       "1                0.0                  -3.0              -1.500000   \n",
       "2                0.0                 -10.0              -5.000000   \n",
       "3                0.0                 -10.0              -3.333333   \n",
       "4                0.0                  -6.0              -6.000000   \n",
       "\n",
       "   user_field14_new_max  user_field14_new_min  user_field1_new_sum  \\\n",
       "0                  -5.0                  -5.0                 -8.0   \n",
       "1                  -1.0                  -2.0                -10.0   \n",
       "2                  -5.0                  -5.0                -15.0   \n",
       "3                  -3.0                  -4.0                -14.0   \n",
       "4                  -6.0                  -6.0                 -9.0   \n",
       "\n",
       "   user_field1_new_mean  user_field1_new_max  user_field1_new_min  \\\n",
       "0             -8.000000                 -8.0                 -8.0   \n",
       "1             -5.000000                 -5.0                 -5.0   \n",
       "2             -7.500000                 -7.0                 -8.0   \n",
       "3             -4.666667                 -3.0                 -6.0   \n",
       "4             -9.000000                 -9.0                 -9.0   \n",
       "\n",
       "   userid_orders  \n",
       "0              1  \n",
       "1              2  \n",
       "2              2  \n",
       "3              3  \n",
       "4              1  \n",
       "\n",
       "[5 rows x 198 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = train[original_columns].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_smth(original, desc, name):\n",
    "    features_names = [\n",
    "        'max',\n",
    "        'min',\n",
    "        'mean_std',\n",
    "        '25',\n",
    "        '50',\n",
    "        '75'\n",
    "    ]\n",
    "    features = pd.concat([\n",
    "        desc['max'] - original,\n",
    "        original - desc['min'],\n",
    "        original - desc['mean'],\n",
    "        (original - desc['mean']) / desc['std'],\n",
    "        (original > desc['25%']).astype(np.float32),\n",
    "        (original > desc['50%']).astype(np.float32),\n",
    "        (original > desc['75%']).astype(np.float32),\n",
    "    ], axis=1, keys=[f'{name}_{features_name}' for features_name in features_names])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:47<00:00,  1.44s/it]\n"
     ]
    }
   ],
   "source": [
    "for column in tqdm(description.columns):\n",
    "    if column == 'orderid':\n",
    "        continue\n",
    "    features = calculate_smth(train[column], description[column], column)\n",
    "    train = pd.concat([train, features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [01:55<00:00,  3.35s/it]\n"
     ]
    }
   ],
   "source": [
    "for column in tqdm(description.columns):\n",
    "    if column == 'orderid':\n",
    "        continue\n",
    "    features = calculate_smth(test[column], description[column], column)\n",
    "    test = pd.concat([test, features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(196056, 438)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(455011, 438)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train_fullest.csv', index=False)\n",
    "test.to_csv('test_fullest.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lgb cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'n_estimators': 800,\n",
    "    'num_leaves':6,\n",
    "    'learning_rate': 0.1,\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:430: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tcv_agg's auc: 0.693788 + 0.00905307\n",
      "[200]\tcv_agg's auc: 0.698901 + 0.00955371\n",
      "[300]\tcv_agg's auc: 0.700666 + 0.010541\n",
      "[400]\tcv_agg's auc: 0.701058 + 0.0115304\n",
      "[500]\tcv_agg's auc: 0.700743 + 0.0110684\n",
      "[600]\tcv_agg's auc: 0.700132 + 0.0111438\n",
      "[700]\tcv_agg's auc: 0.698685 + 0.0113808\n",
      "[800]\tcv_agg's auc: 0.697757 + 0.010284\n"
     ]
    }
   ],
   "source": [
    "w = lgb.cv(parameters, \n",
    "           lgb.Dataset(train.drop(['userid', 'orderid'], axis=1), label=target),\n",
    "           stratified=False,\n",
    "           num_boost_round=800,\n",
    "           nfold=4,\n",
    "           verbose_eval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(333, 0.7015319570036302)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parameters = {\n",
    "#     'n_estimators': 1000,\n",
    "#     'num_leaves':6,\n",
    "#     'learning_rate': 0.1,\n",
    "#     'objective': 'binary',\n",
    "#     'metric': 'auc'\n",
    "# }\n",
    "\n",
    "best_n_estimators = np.argmax(w['auc-mean'])\n",
    "best_n_estimators, w['auc-mean'][best_n_estimators]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "               importance_type='split', learning_rate=0.1, max_depth=-1,\n",
       "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "               n_estimators=333, n_jobs=-1, num_leaves=6, objective=None,\n",
       "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LGBMClassifier(n_estimators=best_n_estimators, num_leaves=6)\n",
    "classifier.fit(train.drop(['orderid', 'userid'], axis=1), target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = classifier.predict_proba(test.drop(['orderid', 'userid'], axis=1))\n",
    "test['proba'] = prediction[:, 1]\n",
    "baseline = test[['orderid', 'proba']]\n",
    "baseline.to_csv('before_feature_selection_user_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=LGBMClassifier(boosting_type='gbdt',\n",
       "                                         class_weight=None,\n",
       "                                         colsample_bytree=1.0,\n",
       "                                         importance_type='split',\n",
       "                                         learning_rate=0.1, max_depth=-1,\n",
       "                                         min_child_samples=20,\n",
       "                                         min_child_weight=0.001,\n",
       "                                         min_split_gain=0.0, n_estimators=333,\n",
       "                                         n_jobs=-1, num_leaves=6,\n",
       "                                         objective=None, random_state=None,\n",
       "                                         reg_alpha=0.0, reg_lambda=0.0,\n",
       "                                         silent=True, subsample=1.0,\n",
       "                                         subsample_for_bin=200000,\n",
       "                                         subsample_freq=0),\n",
       "                max_features=150, norm_order=1, prefit=False, threshold=None)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LGBMClassifier(n_estimators=best_n_estimators, num_leaves=6)\n",
    "\n",
    "embeded_lgb_selector = SelectFromModel(classifier, max_features=150)\n",
    "embeded_lgb_selector.fit(train.drop(['userid', 'orderid'], axis=1), target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeded_lgb_support = embeded_lgb_selector.get_support()\n",
    "embeded_lgb_feature = train.drop(['userid', 'orderid'], axis=1).loc[:,embeded_lgb_support].columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122 selected features\n"
     ]
    }
   ],
   "source": [
    "print(str(len(embeded_lgb_feature)), 'selected features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['field0',\n",
       " 'field1',\n",
       " 'field2',\n",
       " 'field3',\n",
       " 'field4',\n",
       " 'field6',\n",
       " 'field10',\n",
       " 'field11',\n",
       " 'field12',\n",
       " 'field13',\n",
       " 'field14',\n",
       " 'field16',\n",
       " 'field17',\n",
       " 'field18',\n",
       " 'field21',\n",
       " 'field22',\n",
       " 'field23',\n",
       " 'field27',\n",
       " 'field28',\n",
       " 'indicator_goal22',\n",
       " 'indicator_goal24',\n",
       " 'user_field0_sum',\n",
       " 'user_field0_mean',\n",
       " 'user_field0_max',\n",
       " 'user_field1_sum',\n",
       " 'user_field1_mean',\n",
       " 'user_field1_max',\n",
       " 'user_field1_min',\n",
       " 'user_field2_sum',\n",
       " 'user_field2_mean',\n",
       " 'user_field2_max',\n",
       " 'user_field3_sum',\n",
       " 'user_field3_mean',\n",
       " 'user_field3_max',\n",
       " 'user_field3_min',\n",
       " 'user_field5_mean',\n",
       " 'user_field6_sum',\n",
       " 'user_field6_mean',\n",
       " 'user_field6_max',\n",
       " 'user_field6_min',\n",
       " 'user_field7_mean',\n",
       " 'user_field8_mean',\n",
       " 'user_field9_sum',\n",
       " 'user_field9_mean',\n",
       " 'user_field10_sum',\n",
       " 'user_field10_mean',\n",
       " 'user_field11_sum',\n",
       " 'user_field11_mean',\n",
       " 'user_field11_max',\n",
       " 'user_field11_min',\n",
       " 'user_field12_sum',\n",
       " 'user_field12_mean',\n",
       " 'user_field12_max',\n",
       " 'user_field12_min',\n",
       " 'user_field13_sum',\n",
       " 'user_field13_mean',\n",
       " 'user_field13_max',\n",
       " 'user_field13_min',\n",
       " 'user_field14_sum',\n",
       " 'user_field14_mean',\n",
       " 'user_field14_max',\n",
       " 'user_field14_min',\n",
       " 'user_field15_mean',\n",
       " 'user_field16_sum',\n",
       " 'user_field16_mean',\n",
       " 'user_field16_max',\n",
       " 'user_field16_min',\n",
       " 'user_field17_sum',\n",
       " 'user_field17_mean',\n",
       " 'user_field17_max',\n",
       " 'user_field17_min',\n",
       " 'user_field18_sum',\n",
       " 'user_field18_mean',\n",
       " 'user_field18_max',\n",
       " 'user_field18_min',\n",
       " 'user_field19_mean',\n",
       " 'user_field20_sum',\n",
       " 'user_field20_mean',\n",
       " 'user_field20_max',\n",
       " 'user_field20_min',\n",
       " 'user_field21_sum',\n",
       " 'user_field21_mean',\n",
       " 'user_field22_sum',\n",
       " 'user_field22_mean',\n",
       " 'user_field22_max',\n",
       " 'user_field22_min',\n",
       " 'user_field23_sum',\n",
       " 'user_field23_mean',\n",
       " 'user_field23_max',\n",
       " 'user_field23_min',\n",
       " 'user_field24_mean',\n",
       " 'user_field24_max',\n",
       " 'user_field25_sum',\n",
       " 'user_field25_mean',\n",
       " 'user_field25_max',\n",
       " 'user_field25_min',\n",
       " 'user_field26_sum',\n",
       " 'user_field26_mean',\n",
       " 'user_field26_max',\n",
       " 'user_field26_min',\n",
       " 'user_field27_sum',\n",
       " 'user_field27_mean',\n",
       " 'user_field27_max',\n",
       " 'user_field28_sum',\n",
       " 'user_field28_mean',\n",
       " 'user_field29_mean',\n",
       " 'user_indicator_goal23_sum',\n",
       " 'user_indicator_goal23_mean',\n",
       " 'user_indicator_goal24_mean',\n",
       " 'user_indicator_goal25_mean',\n",
       " 'user_months_delation_mean',\n",
       " 'user_field14_new_sum',\n",
       " 'user_field1_new_sum',\n",
       " 'field0_max',\n",
       " 'field1_max',\n",
       " 'field6_max',\n",
       " 'field14_max',\n",
       " 'field16_max',\n",
       " 'field20_max',\n",
       " 'field22_max',\n",
       " 'field23_max',\n",
       " 'field24_max']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeded_lgb_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lgb cv on selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'n_estimators': 800,\n",
    "    'num_leaves':6,\n",
    "    'learning_rate': 0.1,\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tcv_agg's auc: 0.694517 + 0.00966461\n",
      "[200]\tcv_agg's auc: 0.698491 + 0.00937874\n",
      "[300]\tcv_agg's auc: 0.700318 + 0.00933857\n",
      "[400]\tcv_agg's auc: 0.70023 + 0.0107522\n",
      "[500]\tcv_agg's auc: 0.700115 + 0.0107216\n",
      "[600]\tcv_agg's auc: 0.698535 + 0.0108983\n",
      "[700]\tcv_agg's auc: 0.697864 + 0.0103237\n",
      "[800]\tcv_agg's auc: 0.695648 + 0.0101573\n"
     ]
    }
   ],
   "source": [
    "w_selected = lgb.cv(parameters, \n",
    "           lgb.Dataset(train[embeded_lgb_feature], label=target),\n",
    "           stratified=False,\n",
    "           num_boost_round=800,\n",
    "           nfold=4,\n",
    "           verbose_eval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(362, 0.7005269315091297)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_n_estimators_selected = np.argmax(w_selected['auc-mean'])\n",
    "best_n_estimators_selected, w_selected['auc-mean'][best_n_estimators_selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "               importance_type='split', learning_rate=0.1, max_depth=-1,\n",
       "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "               n_estimators=362, n_jobs=-1, num_leaves=6, objective=None,\n",
       "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LGBMClassifier(n_estimators=best_n_estimators_selected, num_leaves=6)\n",
    "classifier.fit(train[embeded_lgb_feature], target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = classifier.predict_proba(test[embeded_lgb_feature])\n",
    "test['proba'] = prediction[:, 1]\n",
    "baseline = test[['orderid', 'proba']]\n",
    "baseline.to_csv('after_feature_selection_user_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['proba_lgb'] = test['proba']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_classifier = RandomForestClassifier(1000)\n",
    "rf_classifier.fit(train[embeded_lgb_feature], target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = rf_classifier.predict_proba(test[embeded_lgb_feature])\n",
    "test['proba_rf'] = prediction[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   50.9s remaining:    0.0s\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  2.5min finished\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegressionCV(cv=3, random_state=0, scoring='roc_auc', verbose=2).fit(train[embeded_lgb_feature], target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression().fit(train[embeded_lgb_feature], target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = clf.predict_proba(test[embeded_lgb_feature])\n",
    "test['proba_logreg'] = prediction[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.78129844]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([test.proba_lgb], [test.proba_rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.77777042]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([test.proba_lgb], [test.proba_logreg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.73088277]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([test.proba_rf], [test.proba_logreg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random blend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_coeff = 0.9\n",
    "rf_coeff = 0.07\n",
    "logreg_coeff = 0.03\n",
    "\n",
    "assert lgb_coeff + rf_coeff + logreg_coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['proba'] = lgb_coeff * test['proba_lgb'] + rf_coeff * test['proba_rf'] + logreg_coeff * test['proba_logreg']\n",
    "baseline = test[['orderid', 'proba']]\n",
    "baseline.to_csv('after_feature_selection_user_features_rf_logreg2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
